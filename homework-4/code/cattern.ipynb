{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "717c0518",
   "metadata": {},
   "source": [
    "# `neural_net.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef12664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcae634",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "  \"\"\"\n",
    "  A two-layer fully-connected neural network. The net has an input dimension of\n",
    "  N, a hidden layer dimension of H, and performs classification over C classes.\n",
    "  We train the network with a softmax loss function and L2 regularization on the\n",
    "  weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "  connected layer.\n",
    "\n",
    "  In other words, the network has the following architecture:\n",
    "\n",
    "  input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "  The outputs of the second fully-connected layer are the scores for each class.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "    \"\"\"\n",
    "    Initialize the model. Weights are initialized to small random values and\n",
    "    biases are initialized to zero. Weights and biases are stored in the\n",
    "    variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "    W1: First layer weights; has shape (D, H)\n",
    "    b1: First layer biases; has shape (H,)\n",
    "    W2: Second layer weights; has shape (H, C)\n",
    "    b2: Second layer biases; has shape (C,)\n",
    "\n",
    "    Inputs:\n",
    "    - input_size: The dimension D of the input data.\n",
    "    - hidden_size: The number of neurons H in the hidden layer.\n",
    "    - output_size: The number of classes C.\n",
    "    \"\"\"\n",
    "    self.params = {}\n",
    "    self.params[\"W1\"] = std * np.random.randn(input_size, hidden_size)\n",
    "    self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "    self.params[\"W2\"] = std * np.random.randn(hidden_size, output_size)\n",
    "    self.params[\"b2\"] = np.zeros(output_size)\n",
    "\n",
    "\n",
    "  def loss(self, X, y=None, reg=0.0):\n",
    "    \"\"\"\n",
    "    Compute the loss and gradients for a two layer fully connected neural network.\n",
    "\n",
    "    Inputs:\n",
    "    - X: Input data of shape (N, D). Each X[i] is a training sample.\n",
    "    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is\n",
    "      an integer in the range 0 <= y[i] < C. This parameter is optional; if it\n",
    "      is not passed then we only return scores, and if it is passed then we\n",
    "      instead return the loss and gradients.\n",
    "    - reg: Regularization strength.\n",
    "\n",
    "    Returns:\n",
    "    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is\n",
    "    the score for class c on input X[i].\n",
    "\n",
    "    If y is not None, instead return a tuple of:\n",
    "    - loss: Loss (data loss and regularization loss) for this batch of training\n",
    "      samples.\n",
    "    - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "      with respect to the loss function; has the same keys as self.params.\n",
    "    \"\"\"\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = self.params[\"W1\"], self.params[\"b1\"]\n",
    "    W2, b2 = self.params[\"W2\"], self.params[\"b2\"]\n",
    "    N, _ = X.shape\n",
    "\n",
    "    ############################################################################\n",
    "    # T5: Perform the forward pass, computing the class scores for the input.  #\n",
    "    # Store the result in the scores variable, which should be an array of     #\n",
    "    # shape (N, C). Note that this does not include the softmax                #\n",
    "    # HINT: This is just a series of matrix multiplication.                    #\n",
    "    ############################################################################\n",
    "    \n",
    "    # Compute the forward pass\n",
    "    X1 = X @ W1 + b1\n",
    "    X1 = np.maximum(0, X1)\n",
    "    scores = X1 @ W2 + b2\n",
    "    \n",
    "    ############################################################################\n",
    "    #                                 END OF T5                                #\n",
    "    ############################################################################\n",
    "    \n",
    "    # If the targets are not given then jump out, we're done\n",
    "    if y is None:\n",
    "      return scores\n",
    "    \n",
    "    ############################################################################\n",
    "    # T6: Finish the forward pass, and compute the loss. This should include   #\n",
    "    # both the data loss and L2 regularization for W1 and W2. Store the result #\n",
    "    # in the variable loss, which should be a scalar. Use the Softmax          #\n",
    "    # classifier loss.                                                         #\n",
    "    ############################################################################\n",
    "    \n",
    "    # Compute the loss\n",
    "    scores_shifted  = scores - np.max(scores, axis=1, keepdims=True)\n",
    "    scores_softmax  = np.exp(scores_shifted)\n",
    "    scores_softmax /= np.sum(scores_softmax, axis=1, keepdims=True)\n",
    "    \n",
    "    loss  = -np.sum(np.log(scores_softmax[np.arange(N), y]))\n",
    "    loss /= N\n",
    "    loss += 1/2 * reg * ((W1 * W1).sum() + (W2 * W2).sum())\n",
    "    \n",
    "    ############################################################################\n",
    "    #                                 END OF T6                                #\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    # T7: Compute the backward pass, computing derivatives of the weights      #\n",
    "    # and biases. Store the results in the grads dictionary. For example,      #\n",
    "    # grads[\"W1\"] should store the gradient on W1, and be a matrix of same     #\n",
    "    # size don't forget about the regularization term                          #\n",
    "    ############################################################################\n",
    "    \n",
    "    # Backward Pass: Compute Gradients\n",
    "    grads = {}\n",
    "    \n",
    "    dscores = scores_softmax\n",
    "    dscores[np.arange(N), y] -= 1\n",
    "    dscores /= N\n",
    "    \n",
    "    grads[\"W2\"] = X1.T @ dscores + reg * W2\n",
    "    grads[\"b2\"] = np.sum(dscores, axis=0)\n",
    "    \n",
    "    dX1 = dscores @ W2.T\n",
    "    dX1[X1 <= 0] = 0\n",
    "    \n",
    "    grads[\"W1\"] = X.T @ dX1 + reg * W1\n",
    "    grads[\"b1\"] = np.sum(dX1, axis=0)\n",
    "\n",
    "    ############################################################################\n",
    "    #                                 END OF T7                                #\n",
    "    ############################################################################\n",
    "\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "  def train(\n",
    "    self,\n",
    "    X, y, X_val, y_val,\n",
    "    learning_rate=1e-3,\n",
    "    learning_rate_decay=0.95,\n",
    "    reg=5e-6,\n",
    "    num_iters=100,\n",
    "    batch_size=200,\n",
    "    verbose=False\n",
    "  ):\n",
    "    \"\"\"\n",
    "    Train this neural network using stochastic gradient descent.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving training data.\n",
    "    - y: A numpy array of shape (N,) giving training labels; y[i] = c means that\n",
    "      X[i] has label c, where 0 <= c < C.\n",
    "    - X_val: A numpy array of shape (N_val, D) giving validation data.\n",
    "    - y_val: A numpy array of shape (N_val,) giving validation labels.\n",
    "    - learning_rate: Scalar giving learning rate for optimization.\n",
    "    - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "      after each epoch.\n",
    "    - reg: Scalar giving regularization strength.\n",
    "    - num_iters: Number of steps to take when optimizing.\n",
    "    - batch_size: Number of training examples to use per step.\n",
    "    - verbose: boolean; if true print progress during optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_train = X.shape[0]\n",
    "    iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "    # Use SGD to optimize the parameters in self.model\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for it in range(num_iters):\n",
    "      #########################################################################\n",
    "      # T8: Create a random minibatch of training data and labels, storing    #\n",
    "      # them in X_batch and y_batch respectively.                             #\n",
    "      # You might find np.random.choice() helpful.                            #\n",
    "      #########################################################################\n",
    "\n",
    "      random_mask = np.random.choice(num_train, batch_size, replace=True)\n",
    "      X_batch = X[random_mask]\n",
    "      y_batch = y[random_mask]\n",
    "      \n",
    "      #########################################################################\n",
    "      #                            END OF YOUR T8                             #\n",
    "      #########################################################################\n",
    "\n",
    "      # Compute loss and gradients using the current minibatch\n",
    "      loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "      loss_history.append(loss)\n",
    "\n",
    "      #########################################################################\n",
    "      # T9: Use the gradients in the grads dictionary to update the           #\n",
    "      # parameters of the network (stored in the dictionary self.params)      #\n",
    "      # using stochastic gradient descent. You'll need to use the gradients   #\n",
    "      # stored in the grads dictionary defined above.                         #\n",
    "      #########################################################################\n",
    "\n",
    "      self.params[\"W1\"] -= learning_rate * grads[\"W1\"]\n",
    "      self.params[\"b1\"] -= learning_rate * grads[\"b1\"]\n",
    "      \n",
    "      self.params[\"W2\"] -= learning_rate * grads[\"W2\"]\n",
    "      self.params[\"b2\"] -= learning_rate * grads[\"b2\"]\n",
    "      \n",
    "      #########################################################################\n",
    "      #                            END OF YOUR T9                             #\n",
    "      #########################################################################\n",
    "\n",
    "      if verbose and it % 100 == 0:\n",
    "        print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n",
    "\n",
    "      # Every epoch, check train and val accuracy and decay learning rate.\n",
    "      if it % iterations_per_epoch == 0:\n",
    "        # Check accuracy\n",
    "        train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "        val_acc = (self.predict(X_val) == y_val).mean()\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        #######################################################################\n",
    "        # T10: Decay learning rate (exponentially) after each epoch           #\n",
    "        #######################################################################\n",
    "\n",
    "        # Decay learning rate\n",
    "        learning_rate *= learning_rate_decay\n",
    "        \n",
    "        #######################################################################\n",
    "        #                           END OF YOUR T10                           #\n",
    "        #######################################################################\n",
    "        \n",
    "\n",
    "    return {\n",
    "      \"loss_history\": loss_history,\n",
    "      \"train_acc_history\": train_acc_history,\n",
    "      \"val_acc_history\": val_acc_history,\n",
    "    }\n",
    "\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this two-layer network to predict labels for\n",
    "    data points. For each data point we predict scores for each of the C\n",
    "    classes, and assign each data point to the class with the highest score.\n",
    "\n",
    "    Inputs:\n",
    "    - X: A numpy array of shape (N, D) giving N D-dimensional data points to\n",
    "      classify.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: A numpy array of shape (N,) giving predicted labels for each of\n",
    "      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted\n",
    "      to have class c, where 0 <= c < C.\n",
    "    \"\"\"\n",
    "\n",
    "    ###########################################################################\n",
    "    # T11: Implement this function; it should be VERY simple!                 #\n",
    "    ###########################################################################\n",
    "\n",
    "    scores = np.array(self.loss(X))\n",
    "    y_pred = np.argmax(scores, axis=1)\n",
    "\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR T11                             #\n",
    "    ###########################################################################\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab4d8bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58428be5",
   "metadata": {},
   "source": [
    "# `data_utils.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f468c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "article_types = [\"article\", \"encyclopedia\", \"news\", \"novel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a695ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_words(files):\n",
    "    \"\"\"\n",
    "    Transform list of files to list of words,\n",
    "    removing new line character\n",
    "    and replace name entity \"<NE>...</NE>\" and abbreviation \"<AB>...</AB>\" symbol\n",
    "    \"\"\"\n",
    "\n",
    "    repls = {\"<NE>\" : \"\",\"</NE>\" : \"\",\"<AB>\": \"\",\"</AB>\": \"\"}\n",
    "\n",
    "    words_all = []\n",
    "    for _, file in enumerate(files):\n",
    "        lines = open(file, \"r\")\n",
    "        for line in lines:\n",
    "            line = reduce(lambda a, kv: a.replace(*kv), repls.items(), line)\n",
    "            words = [word for word in line.split(\"|\") if word is not \"\\n\"]\n",
    "            words_all.extend(words)\n",
    "            \n",
    "    return words_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "308b298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_char_dataframe(words):\n",
    "    \"\"\"\n",
    "    Give list of input tokenized words,\n",
    "    create dataframe of characters where first character of\n",
    "    the word is tagged as 1, otherwise 0\n",
    "    Example\n",
    "    =======\n",
    "    [\"กิน\", \"หมด\"] to dataframe of\n",
    "    [{\"char\": \"ก\", \"type\": ..., \"target\": 1}, ...,\n",
    "     {\"char\": \"ด\", \"type\": ..., \"target\": 0}]\n",
    "    \"\"\"\n",
    "    \n",
    "    char_dict = []\n",
    "    for word in words:\n",
    "        for i, char in enumerate(word):\n",
    "            if i == 0:\n",
    "                char_dict.append({\"char\": char, \"target\": True})\n",
    "            else:\n",
    "                char_dict.append({\"char\": char, \"target\": False})\n",
    "                \n",
    "    return pd.DataFrame(char_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef5fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best_dataset(best_path, output_path=\"cleaned_data\", create_val=False):\n",
    "    \"\"\"\n",
    "    Generate CSV file for training and testing data\n",
    "    Input\n",
    "    =====\n",
    "    best_path: str, path to BEST folder which contains unzipped subfolder\n",
    "        \"article\", \"encyclopedia\", \"news\", \"novel\"\n",
    "    cleaned_data: str, path to output folder, the cleaned data will be saved\n",
    "        in the given folder name where training set will be stored in `train` folder\n",
    "        and testing set will be stored on `test` folder\n",
    "    create_val: boolean, True or False, if True, divide training set into training set and\n",
    "        validation set in `val` folder\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(output_path):\n",
    "        os.mkdir(output_path)\n",
    "        \n",
    "    if not os.path.isdir(os.path.join(output_path, \"train\")):\n",
    "        os.makedirs(os.path.join(output_path, \"train\"))\n",
    "        \n",
    "    if not os.path.isdir(os.path.join(output_path, \"test\")):\n",
    "        os.makedirs(os.path.join(output_path, \"test\"))\n",
    "        \n",
    "    if not os.path.isdir(os.path.join(output_path, \"val\")) and create_val:\n",
    "        os.makedirs(os.path.join(output_path, \"val\"))\n",
    "\n",
    "    for article_type in article_types:\n",
    "        files = glob(os.path.join(best_path, article_type, \"*.txt\"))\n",
    "        files_train, files_test = train_test_split(files, random_state=0, test_size=0.1)\n",
    "        \n",
    "        if create_val:\n",
    "            files_train, files_val = train_test_split(files_train, random_state=0, test_size=0.1)\n",
    "            val_words = generate_words(files_val)\n",
    "            val_df = create_char_dataframe(val_words)\n",
    "            val_df.to_csv(os.path.join(output_path, \"val\", \"df_best_{}_val.csv\".format(article_type)), index=False)\n",
    "            \n",
    "        train_words = generate_words(files_train)\n",
    "        test_words = generate_words(files_test)\n",
    "        train_df = create_char_dataframe(train_words)\n",
    "        test_df = create_char_dataframe(test_words)\n",
    "        \n",
    "        train_df.to_csv(os.path.join(output_path, \"train\", \"df_best_{}_train.csv\".format(article_type)), index=False)\n",
    "        test_df.to_csv(os.path.join(output_path, \"test\", \"df_best_{}_test.csv\".format(article_type)), index=False)\n",
    "        \n",
    "        print(\"Save {} to CSV file\".format(article_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f37039",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cb2861",
   "metadata": {},
   "source": [
    "# `gradient_check.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707b1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "227d3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=True, h=0.00001):\n",
    "  \"\"\" \n",
    "  A naive implementation of numerical gradient of f at x.\n",
    "  - f should be a function that takes a single argument\n",
    "  - x is the point (numpy array) to evaluate the gradient at\n",
    "  \"\"\" \n",
    "  \n",
    "  # Evaluate function value at original point.\n",
    "  fx = f(x)\n",
    "  grad = np.zeros_like(x)\n",
    "  \n",
    "  # Iterate over all indexes in x.\n",
    "  it = np.nditer(x, flags=[\"multi_index\"], op_flags=[[\"readwrite\"]])\n",
    "  \n",
    "  while not it.finished:\n",
    "    # Evaluate function at x + h.\n",
    "    ix = it.multi_index\n",
    "    oldval = x[ix]\n",
    "    \n",
    "    # Increment by h.\n",
    "    x[ix] = oldval + h\n",
    "    \n",
    "    # Evalute f(x + h).\n",
    "    fxph = f(x) \n",
    "    x[ix] = oldval - h\n",
    "    \n",
    "    # Evalute f(x - h).\n",
    "    fxmh = f(x)\n",
    "    \n",
    "    # Restore the original value.\n",
    "    x[ix] = oldval\n",
    "\n",
    "    # Compute the partial derivative with centered formula.\n",
    "    grad[ix] = (fxph - fxmh) / (2 * h) # The slope\n",
    "    \n",
    "    if verbose:\n",
    "      print(ix, grad[ix])\n",
    "    \n",
    "    # Step to next dimension\n",
    "    it.iternext()\n",
    "\n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dffa2270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "  \"\"\"\n",
    "  Evaluate a numeric gradient for a function that accepts a numpy\n",
    "  array and returns a numpy array.\n",
    "  \"\"\"\n",
    "  \n",
    "  grad = np.zeros_like(x)\n",
    "  it = np.nditer(x, flags=[\"multi_index\"], op_flags=[[\"readwrite\"]])\n",
    "  \n",
    "  while not it.finished:\n",
    "    ix = it.multi_index\n",
    "    \n",
    "    oldval = x[ix]\n",
    "    x[ix] = oldval + h\n",
    "    \n",
    "    pos = f(x).copy()\n",
    "    x[ix] = oldval - h\n",
    "    \n",
    "    neg = f(x).copy()\n",
    "    x[ix] = oldval\n",
    "    \n",
    "    grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "    \n",
    "    it.iternext()\n",
    "    \n",
    "  return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6188d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_blobs(f, inputs, output, h=1e-5):\n",
    "  \"\"\"\n",
    "  Compute numeric gradients for a function that operates on input\n",
    "  and output blobs.\n",
    "  \n",
    "  We assume that f accepts several input blobs as arguments, followed by a blob\n",
    "  into which outputs will be written. For example, f might be called like this:\n",
    "\n",
    "  f(x, w, out)\n",
    "  \n",
    "  where x and w are input Blobs, and the result of f will be written to out.\n",
    "\n",
    "  Inputs: \n",
    "  - f: function\n",
    "  - inputs: tuple of input blobs\n",
    "  - output: output blob\n",
    "  - h: step size\n",
    "  \"\"\"\n",
    "  numeric_diffs = []\n",
    "  for input_blob in inputs:\n",
    "    diff = np.zeros_like(input_blob.diffs)\n",
    "    \n",
    "    it = np.nditer(input_blob.vals, flags=[\"multi_index\"], op_flags=[[\"readwrite\"]])\n",
    "    \n",
    "    while not it.finished:\n",
    "      idx = it.multi_index\n",
    "      orig = input_blob.vals[idx]\n",
    "\n",
    "      input_blob.vals[idx] = orig + h\n",
    "      \n",
    "      f(*(inputs + (output,)))\n",
    "      \n",
    "      pos = np.copy(output.vals)\n",
    "      input_blob.vals[idx] = orig - h\n",
    "      \n",
    "      f(*(inputs + (output,)))\n",
    "      \n",
    "      neg = np.copy(output.vals)\n",
    "      input_blob.vals[idx] = orig\n",
    "      \n",
    "      diff[idx] = np.sum((pos - neg) * output.diffs) / (2.0 * h)\n",
    "\n",
    "      it.iternext()\n",
    "      \n",
    "    numeric_diffs.append(diff)\n",
    "    \n",
    "  return numeric_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c937bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient_net(net, inputs, output, h=1e-5):\n",
    "  return eval_numerical_gradient_blobs(lambda *args: net.forward(), inputs, output, h=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f52773a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check_sparse(f, x, analytic_grad, num_checks=10, h=1e-5):\n",
    "  \"\"\"\n",
    "  Sample a few random elements and only return numerical\n",
    "  gradients in these dimensions.\n",
    "  \"\"\"\n",
    "\n",
    "  for _ in range(num_checks):\n",
    "    ix = tuple([randrange(m) for m in x.shape])\n",
    "\n",
    "    oldval = x[ix]\n",
    "    \n",
    "    # Increment by h.\n",
    "    x[ix] = oldval + h\n",
    "    \n",
    "    # Evaluate f(x + h).\n",
    "    fxph = f(x)\n",
    "    \n",
    "    # Decrement by h.\n",
    "    x[ix] = oldval - h\n",
    "    \n",
    "    # Evaluate f(x - h).\n",
    "    fxmh = f(x)\n",
    "    \n",
    "    # Restore the original value.\n",
    "    x[ix] = oldval\n",
    "\n",
    "    grad_numerical = (fxph - fxmh) / (2 * h)\n",
    "    grad_analytic = analytic_grad[ix]\n",
    "    rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "    \n",
    "    print(\"numerical: %f analytic: %f, relative error: %e\" % (grad_numerical, grad_analytic, rel_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2794ba",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
