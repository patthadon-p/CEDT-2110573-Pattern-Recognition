\documentclass[a4paper, 10pt]{article}
\usepackage{../CEDT-Homework-style}

\usepackage{amsmath}
\allowdisplaybreaks

\setlength{\headheight}{14.49998pt}

\begin{document}
\subject[2110573 - Pattern Recognition]
\hwtitle{3}{}{Week 4-5: Fisherface}{Patthadon Phengpinij}{ChatGPT}


% ================================================================================ %
\section{Hello Soft Clustering (GMM)}
% ================================================================================ %

Recall from HW1 we did K-means clustering.
Fitting a GMM on a set of points can be considered as another method to do clustering but now with soft assignments.

Consider the same set of points we used in HW1

\[
\renewcommand{\arraystretch}{1.3}
\begin{array}{|c|c|}
    \hline
    x & y \\
    \hline
    1 & 2 \\
    3 & 3 \\
    2 & 2 \\
    8 & 8 \\
    6 & 6 \\
    7 & 7 \\
    -3 & -3 \\
    -2 & -4 \\
    -7 & -7 \\
    \hline
\end{array}
\]

\begin{center}
    \includegraphics[width=0.4\textwidth]{images/clustering_data.png}
\end{center}

In class, we showed that we could fit a GMM on 1-dimensional data by using Expectation Maximization (EM).
The algorithm for doing EM on N-dimensional GMM is very similar.
The exact algorithm is as follows:

\myspace

\textbf{Initialization:}  Initialize the mixture weights, \( \phi = \set{ m_j } \),
where \( j \) is the mixture number, means of each Gaussian, \( \vec{\mu_j} \) (now a vector of \( N \) dimensions),
and covariance matrices of each Gaussian, \( {\bf{\Sigma}}_j \)

\textbf{Expectation:} Find the soft assignments for each data point \( w_{n, j} \) where \( n \) corresponds to the sample index.
\[ w_{n, j} = \frac{p( x_n ; \vec{\mu_j}, {\bf{\Sigma}}_j ) m_j}{\sum_j p( x_n ; \vec{\mu_j}, {\bf{\Sigma}}_j ) m_j} \]
\( w_{n, j} \) means the probability that data point \( n \) comes from Gaussian number \( j \).

\newpage

\textbf{Maximization:} Update the model parameters, \( \phi \), \( \vec{\mu} \), \( {\bf{\Sigma}}_j \).
\[ m_j = \frac{1}{N} \Sigma_n w_{n, j} \]
\[ \vec{\mu_j} = \frac{\Sigma_n w_{n, j} {\vec{x_n}}}{\Sigma_n w_{n, j}} \]
\[ {\bf{\Sigma}}_j = \frac{\Sigma_n w_{n, j} (\vec{x_n} - \vec{\mu_j})(\vec{x_n} - \vec{\mu_j})^T}{\Sigma_n w_{n, j}} \]

The above equation is used for full covariance matrices.
For our small toy example, we will use diagonal covariance matrices, which can be acquired by setting the off-diagonal values to zero.
In other words, \( {\bf{\Sigma}}_{(i, j)} = 0 \), for \( i \neq j \).

% ================================================================================ %
%                                    Problem T0x                                   %
% ================================================================================ %
\begin{Tproblem}
Here's problem statement for \textbf{T} problem.
\begin{codingbox}
print("You can insert code like this")
\end{codingbox}

You can insert sub-problems:
\begin{Tsubproblems}
    \item Here's sub-problem 1.
    \item Here's sub-problem 2.
    \item Here's sub-problem 3.
    \begin{Tsubsubproblems}
        \item Here's sub-sub-problem 1.
        \item Here's sub-sub-problem 2.
    \end{Tsubsubproblems}
\end{Tsubproblems}
\end{Tproblem}

\begin{solution}
The solution goes here.
\end{solution}
% ================================================================================ %


% ================================================================================ %
%                                   Problem OT0x                                   %
% ================================================================================ %
\begin{OTproblem}
Here's the problem statement for \textbf{OT} problem.
\end{OTproblem}

\begin{solution}    
Here's the proof for the \textbf{OT} problem.
\begin{proofbox}
The proof inside the solution box goes here.
\end{proofbox}
\end{solution}
% ================================================================================ %


\end{document}